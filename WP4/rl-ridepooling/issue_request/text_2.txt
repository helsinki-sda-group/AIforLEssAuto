I'm having an issue with my code. During the runtime of the job, sometimes a rather strange error occurs that halts the execution of the entire program. Sadly I'm not able to reproduce the error consistently and it seems to happen at random points in time. For some jobs the error doesn't occur at all, but the longer the job runs and the more CPUs and environments I run in parallel, the more often the error seems to occur. I have attached the example of this error (see eof_error_example.txt). Sometimes the error is accompannied by the message "corrupted double-linked list" or "corrupted size vs. prev_size" right before it.

I have attached the batch file I use for launching the job (see example_job_file.sh) and a python script I use (see train.py). My code is written in python. I installed python through the Singularity container by following this guide: https://lumi-supercomputer.github.io/LUMI-EasyBuild-docs/p/PyTorch/#module-and-wrapper-scripts. My code uses the SubprocVecEnv vectorized environment in stable_baselines3 library, which in turn uses pytorch, and the multiprocessing library to launch multiple instances of the reinforcement learning environment based on SUMO (Simulation of Urban MObility) to simulate traffic data.



The error seems unrelated to the recent LUMI update since I'm also getting an error on other HPC systems. I am using the latest version of the Pytorch container with Python 3.10 and Pytorch 2.2.2 (https://lumi-supercomputer.github.io/LUMI-EasyBuild-docs/p/PyTorch/PyTorch-2.2.2-rocm-5.6.1-python-3.10-vllm-0.4.0.post1-singularity-20240617/) and have updated all my pip libraries to the latest version. The complete list of pip packages can be found in pip_freeze.txt. I've also tried two different versions of LUMI software stack (23.09 and 24.03) and the error appeared in both of them. My training is done entirely on CPU in the "small" partition. 

I was suspecting the error has something to do with OOM, but seff tells me that the jobs are not exceeding memory usage limits.

The job requires a lot of resources to debug since it usually only happens when I use 100+ environments and run the job for at at least a few hours, thus debugging it would take me a lot of resources from my usage quota.





Similar issues: 
https://github.com/DLR-RM/stable-baselines3/issues/1956 (case that looks the most similar to mine)

https://github.com/pytorch/pytorch/issues/45746






files to send:

example_job_file: area3/1_sampled/3_1_256e_128c.sh

pip_freeze.txt

train.py: gym_test-rs.py

fully reproducible small example (with config for area3, demand data for 1 sample rate, map of helsinki)