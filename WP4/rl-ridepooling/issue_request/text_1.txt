I have a question about reserving CPUs on the nodes in Slurm in the job file. I'm sending an example of the job file that I have. (see example_job_file.sh)

My issue is that, for example, when submitting a job that reserves 16 CPUs with --cpus-per-task=16 and running hybrid_check tool, it shows me that sometimes the CPUs are allocated consecutively and in groups (0-15, 16-31, 32-47, 48-63 etc.), and sometimes they seem to be split randomly in between (for example, as in hybrid_check_16_bad.txt, which reserved CPUs 40-55). When running hybrid_check as I do in all my batch scripts, I notice that OpenMP only schedules the threads on CPUs from one NUMA node and doesn't consider other CPUs. This impacts the performance since in the cases when all CPUs are within one NUMA node, OpenMP is able to see all of them. (see hybrid_check_16_good.txt). I get a similar issue for the cases when reserving 8 and 32 CPUs. OpenMP only sees all cores if the allocation completely spans two NUMA nodes, otherwise it will not utilize some of the CPUs. (see other files attached)

Is it somehow possible to guarantee using #SBATCH directives that my job gets allocated on the CPUs that span entire NUMA nodes, without reserving the whole node? I tried to play around with --extra-node-info flag, but it doesn't seem to do anything. Alternatively, what should I do to allow OpenMP to see CPUs from other NUMA nodes?